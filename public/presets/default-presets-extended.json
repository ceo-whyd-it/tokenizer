[
  {
    "name": "OpenAI Comparison",
    "input_text": "The quick brown fox jumps over the lazy dog. This is a test sentence for tokenization comparison.",
    "Tokenizer_1": "cl100k_base",
    "Tokenizer_2": "r50k_base",
    "Tokenizer_3": "o200k_base"
  },
  {
    "name": "Custom vs GPT",
    "input_text": "DobrÃ½ deÅˆ! Ako sa mÃ¡te? Toto je test slovenskej tokenizÃ¡cie.",
    "Tokenizer_1": "custom:Hviezdo 512",
    "Tokenizer_2": "cl100k_base",
    "Tokenizer_3": "custom:Hviezdo LLaMA CulturaX"
  },
  {
    "name": "HuggingFace Models",
    "input_text": "# Code Example\n\ndef fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)",
    "Tokenizer_1": "google/gemma-7b",
    "Tokenizer_2": "microsoft/phi-2",
    "Tokenizer_3": "deepseek-ai/DeepSeek-R1"
  },
  {
    "name": "Multi-Language Test",
    "input_text": "Hello world! ä½ å¥½ä¸–ç•Œ! Ğ—Ğ´Ñ€Ğ°Ğ²ÑÑ‚Ğ²ÑƒĞ¹ Ğ¼Ğ¸Ñ€! Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…! ã“ã‚“ã«ã¡ã¯ä¸–ç•Œï¼",
    "Tokenizer_1": "Qwen/Qwen2.5-72B",
    "Tokenizer_2": "cl100k_base",
    "Tokenizer_3": "google/gemma-7b"
  },
  {
    "name": "Coding Comparison",
    "input_text": "import React, { useState, useEffect } from 'react'\n\nconst TokenizerApp = () => {\n  const [text, setText] = useState('')\n  const [tokens, setTokens] = useState([])\n  \n  useEffect(() => {\n    // Tokenize when text changes\n    tokenizeText(text)\n  }, [text])\n  \n  return <div>{tokens.length} tokens</div>\n}",
    "Tokenizer_1": "deepseek-ai/DeepSeek-R1",
    "Tokenizer_2": "p50k_base",
    "Tokenizer_3": "cl100k_base"
  },
  {
    "name": "All OpenAI Encodings",
    "input_text": "Testing all OpenAI tokenizer encodings: GPT-2, Codex, GPT-3.5/4, Edit models, and GPT-4o variants.",
    "Tokenizer_1": "r50k_base",
    "Tokenizer_2": "p50k_base",
    "Tokenizer_3": "cl100k_base"
  },
  {
    "name": "Slovak Language Focus",
    "input_text": "SlovenskÃ½ jazyk mÃ¡ 46 pÃ­smen v abecede. Obsahuje diakritickÃ© znamienka ako Ã¡, Ä, Ä, Ã©, Ã­, Ä¾, Äº, Åˆ, Ã³, Ã´, Å•, Å¡, Å¥, Ãº, Ã½, Å¾. Toto je test tokenizÃ¡cie pre slovenskÃ© slovÃ¡ a frÃ¡zy.",
    "Tokenizer_1": "custom:Hviezdo 512",
    "Tokenizer_2": "custom:Hviezdo LLaMA CulturaX",
    "Tokenizer_3": "custom:Hviezdo LLaMA All HV 32k"
  },
  {
    "name": "AI Model Comparison",
    "input_text": "Compare tokenization across different AI architectures: Google's Gemma (Transformer), Microsoft's Phi (small model), and Meta's Llama (large-scale).",
    "Tokenizer_1": "google/gemma-7b",
    "Tokenizer_2": "microsoft/phi-2",
    "Tokenizer_3": "meta-llama/Meta-Llama-3-8B"
  },
  {
    "name": "Berry Spelling Trap (why LLMs misspell)",
    "input_text": "raspberry strawberry blueberry\nraspbery (missing r)\nra sp berry (inserted spaces)\nRASPBERRY vs raspberry",
    "Tokenizer_1": "cl100k_base",
    "Tokenizer_2": "r50k_base",
    "Tokenizer_3": "llama3"
  },
  {
    "name": "Grapheme Chaos (reversing strings is hard)",
    "input_text": "naÃ¯ve / naive\ncafÃ© / cafeÌ  (combining acute)\nÄ¾alia / lÌŒalia  (combining caron)\nğŸ‘ / ğŸ‘ğŸ½\nğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦  (ZWJ family)",
    "Tokenizer_1": "o200k_base",
    "Tokenizer_2": "cl100k_base",
    "Tokenizer_3": "llama3"
  },
  {
    "name": "Japanese No-Spaces (nonâ€‘English penalty)",
    "input_text": "å¯¿å¸ãŒé£Ÿã¹ãŸã„ã€‚æ±äº¬ã§ãŠã™ã™ã‚ã¯ï¼Ÿãƒ©ãƒ¼ãƒ¡ãƒ³ã‚‚å¥½ãã§ã™ã€‚",
    "Tokenizer_1": "cl100k_base",
    "Tokenizer_2": "r50k_base",
    "Tokenizer_3": "Qwen/Qwen2.5-72B"
  },
  {
    "name": "Numbers & Digits (arithmetic token soup)",
    "input_text": "1+1=2\n10,000 vs 10000 vs 10Â 000\n3.141592653589793238462643383279\n1234567891011121314151617181920",
    "Tokenizer_1": "cl100k_base",
    "Tokenizer_2": "r50k_base",
    "Tokenizer_3": "google/gemma-7b"
  },
  {
    "name": "Special Token Landmines (<|endoftext|>, FIM)",
    "input_text": "Start <|endoftext|> middle <|fim_prefix|> end",
    "Tokenizer_1": "r50k_base",
    "Tokenizer_2": "p50k_edit",
    "Tokenizer_3": "cl100k_base"
  },
  {
    "name": "SolidGoldMagikarp (rare merges get weird)",
    "input_text": "SolidGoldMagikarp\npetertodd\nLeilan\nTheNitromeFan",
    "Tokenizer_1": "cl100k_base",
    "Tokenizer_2": "r50k_base",
    "Tokenizer_3": "llama3"
  },
  {
    "name": "Whitespace Isnâ€™t One Thing",
    "input_text": "word word (space)\nwordÂ word (NBSP)\nwordâ€‹word (ZWSP)\nword\tword (TAB)\nword\nword (newline)",
    "Tokenizer_1": "cl100k_base",
    "Tokenizer_2": "r50k_base",
    "Tokenizer_3": "o200k_base"
  },
  {
    "name": "Slovak: Precomposed vs Combining",
    "input_text": "Ã¡ / aÌ\nÄ / cÌŒ\nÄ / dÌŒ\nÅ¥ / tÌŒ\nÃ´ / oÌ‚\nÄ¾ / lÌŒ",
    "Tokenizer_1": "custom:Hviezdo 512",
    "Tokenizer_2": "cl100k_base",
    "Tokenizer_3": "custom:Hviezdo LLaMA CulturaX"
  },
  {
    "name": "Homoglyphs (Latin vs Cyrillic lookâ€‘alikes)",
    "input_text": "a (Latin) vs Ğ° (Cyrillic U+0430)\ne (Latin) vs Ğµ (Cyrillic U+0435)\no (Latin) vs Ğ¾ (Cyrillic U+043E)",
    "Tokenizer_1": "cl100k_base",
    "Tokenizer_2": "llama3",
    "Tokenizer_3": "google/gemma-7b"
  },
  {
    "name": "URLs, Paths & Punctuation",
    "input_text": "https://example.com/path/to/file?x=1&y=2#frag\nC:\\\\Program Files\\\\App\\\\bin.exe\n./../src/index.js",
    "Tokenizer_1": "p50k_base",
    "Tokenizer_2": "cl100k_base",
    "Tokenizer_3": "deepseek-ai/DeepSeek-R1"
  },
  {
    "name": "Hviezdoslav",
    "input_text": "Bola Å¾atva. SotvaÅ¾e hojnÃ¡ oÅ¥aÅ¾ rosy\nobschla: zrazu zmihalo tisÃ­c skvÃºcich kosÃ­\nvÃ´kol jasnÃ½ch po strÃ¡Åˆach nad ozimÃ­n diely,\nnakovanÃ½ch za svitu, leda prÃºÅ¾ok biely\nnakazoval, kade mÃ¡ slnce zpoza holÃ­ â€”\n(lebo planÃ½ gazda to, ktorÃ½ kÃºva v poli;\neÅ¡te horÅ¡Ã­, oslou len Äo vÅ¾dy midlikuje:\nnepodlieha Ãºroda, len sa chlpÃ­, snuje).\nLigot zahral, hovorÃ­m â€” rozbresknutÃ¡ zora â€”\nvÅ¡ak i zhasol: do spÄ¾aslÃ½ch klasnatÃ©ho mora\nvÄºn vtopil sa po lÃºÄi, kde Ãºvrate zhusta;\nnebadaÅ¥ ho, iba ÄuÅ¥: jak sa valÃ­, Å¡Ãºsta\nbodrÃ½m koscom zajatÃ¡ ruÄaj zo pÅ¡enice\nÄi lenivÃ½ Å¾ita prÃºd. ZhÃ­kly prepelice\n,chau-chauâ€˜ v strachu nemalom na ruch zakosenia,\nhybaj s rÃ¡znym ,podspodok!â€˜ prosto do jaÄmeÅˆa!\nChriaÅ¡teÄ¾ zvrieskal; svrÄky zas, ach, tak smutno nÃ´Å¥a,\nnie div: keÄ im vezmÃº Å¾eÅˆ, Äo im zbude? Psota.",
    "Tokenizer_1": "gpt2",
    "Tokenizer_2": "cl100k_base",
    "Tokenizer_3": "custom:Hviezdo LLaMA CulturaX"
  }
]