[
  {
    "name": "OpenAI Comparison",
    "input_text": "The quick brown fox jumps over the lazy dog. This is a test sentence for tokenization comparison.",
    "Tokenizer_1": "cl100k_base",
    "Tokenizer_2": "r50k_base",
    "Tokenizer_3": "o200k_base"
  },
  {
    "name": "Custom vs GPT",
    "input_text": "Dobrý deň! Ako sa máte? Toto je test slovenskej tokenizácie.",
    "Tokenizer_1": "custom:Hviezdo 512",
    "Tokenizer_2": "cl100k_base",
    "Tokenizer_3": "custom:Hviezdo LLaMA CulturaX"
  },
  {
    "name": "HuggingFace Models",
    "input_text": "# Code Example\n\ndef fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)",
    "Tokenizer_1": "google/gemma-7b",
    "Tokenizer_2": "microsoft/phi-2",
    "Tokenizer_3": "deepseek-ai/DeepSeek-R1"
  },
  {
    "name": "Multi-Language Test",
    "input_text": "Hello world! 你好世界! Здравствуй мир! مرحبا بالعالم! こんにちは世界！",
    "Tokenizer_1": "Qwen/Qwen2.5-72B",
    "Tokenizer_2": "cl100k_base",
    "Tokenizer_3": "google/gemma-7b"
  },
  {
    "name": "Coding Comparison",
    "input_text": "import React, { useState, useEffect } from 'react'\n\nconst TokenizerApp = () => {\n  const [text, setText] = useState('')\n  const [tokens, setTokens] = useState([])\n  \n  useEffect(() => {\n    // Tokenize when text changes\n    tokenizeText(text)\n  }, [text])\n  \n  return <div>{tokens.length} tokens</div>\n}",
    "Tokenizer_1": "deepseek-ai/DeepSeek-R1",
    "Tokenizer_2": "p50k_base",
    "Tokenizer_3": "cl100k_base"
  },
  {
    "name": "All OpenAI Encodings",
    "input_text": "Testing all OpenAI tokenizer encodings: GPT-2, Codex, GPT-3.5/4, Edit models, and GPT-4o variants.",
    "Tokenizer_1": "r50k_base",
    "Tokenizer_2": "p50k_base",
    "Tokenizer_3": "cl100k_base"
  },
  {
    "name": "Slovak Language Focus",
    "input_text": "Slovenský jazyk má 46 písmen v abecede. Obsahuje diakritické znamienka ako á, č, ď, é, í, ľ, ĺ, ň, ó, ô, ŕ, š, ť, ú, ý, ž. Toto je test tokenizácie pre slovenské slová a frázy.",
    "Tokenizer_1": "custom:Hviezdo 512",
    "Tokenizer_2": "custom:Hviezdo LLaMA CulturaX",
    "Tokenizer_3": "custom:Hviezdo LLaMA All HV 32k"
  },
  {
    "name": "AI Model Comparison",
    "input_text": "Compare tokenization across different AI architectures: Google's Gemma (Transformer), Microsoft's Phi (small model), and Meta's Llama (large-scale).",
    "Tokenizer_1": "google/gemma-7b",
    "Tokenizer_2": "microsoft/phi-2",
    "Tokenizer_3": "meta-llama/Meta-Llama-3-8B"
  },
  {
    "name": "Berry Spelling Trap (why LLMs misspell)",
    "input_text": "raspberry strawberry blueberry\nraspbery (missing r)\nra sp berry (inserted spaces)\nRASPBERRY vs raspberry",
    "Tokenizer_1": "cl100k_base",
    "Tokenizer_2": "r50k_base",
    "Tokenizer_3": "llama3"
  },
  {
    "name": "Grapheme Chaos (reversing strings is hard)",
    "input_text": "naïve / naive\ncafé / café  (combining acute)\nľalia / ľalia  (combining caron)\n👍 / 👍🏽\n👨‍👩‍👧‍👦  (ZWJ family)",
    "Tokenizer_1": "o200k_base",
    "Tokenizer_2": "cl100k_base",
    "Tokenizer_3": "llama3"
  },
  {
    "name": "Japanese No-Spaces (non‑English penalty)",
    "input_text": "寿司が食べたい。東京でおすすめは？ラーメンも好きです。",
    "Tokenizer_1": "cl100k_base",
    "Tokenizer_2": "r50k_base",
    "Tokenizer_3": "Qwen/Qwen2.5-72B"
  },
  {
    "name": "Numbers & Digits (arithmetic token soup)",
    "input_text": "1+1=2\n10,000 vs 10000 vs 10 000\n3.141592653589793238462643383279\n1234567891011121314151617181920",
    "Tokenizer_1": "cl100k_base",
    "Tokenizer_2": "r50k_base",
    "Tokenizer_3": "google/gemma-7b"
  },
  {
    "name": "Special Token Landmines (<|endoftext|>, FIM)",
    "input_text": "Start <|endoftext|> middle <|fim_prefix|> end",
    "Tokenizer_1": "r50k_base",
    "Tokenizer_2": "p50k_edit",
    "Tokenizer_3": "cl100k_base"
  },
  {
    "name": "SolidGoldMagikarp (rare merges get weird)",
    "input_text": "SolidGoldMagikarp\npetertodd\nLeilan\nTheNitromeFan",
    "Tokenizer_1": "cl100k_base",
    "Tokenizer_2": "r50k_base",
    "Tokenizer_3": "llama3"
  },
  {
    "name": "Whitespace Isn’t One Thing",
    "input_text": "word word (space)\nword word (NBSP)\nword​word (ZWSP)\nword\tword (TAB)\nword\nword (newline)",
    "Tokenizer_1": "cl100k_base",
    "Tokenizer_2": "r50k_base",
    "Tokenizer_3": "o200k_base"
  },
  {
    "name": "Slovak: Precomposed vs Combining",
    "input_text": "á / á\nč / č\nď / ď\nť / ť\nô / ô\nľ / ľ",
    "Tokenizer_1": "custom:Hviezdo 512",
    "Tokenizer_2": "cl100k_base",
    "Tokenizer_3": "custom:Hviezdo LLaMA CulturaX"
  },
  {
    "name": "Homoglyphs (Latin vs Cyrillic look‑alikes)",
    "input_text": "a (Latin) vs а (Cyrillic U+0430)\ne (Latin) vs е (Cyrillic U+0435)\no (Latin) vs о (Cyrillic U+043E)",
    "Tokenizer_1": "cl100k_base",
    "Tokenizer_2": "llama3",
    "Tokenizer_3": "google/gemma-7b"
  },
  {
    "name": "URLs, Paths & Punctuation",
    "input_text": "https://example.com/path/to/file?x=1&y=2#frag\nC:\\\\Program Files\\\\App\\\\bin.exe\n./../src/index.js",
    "Tokenizer_1": "p50k_base",
    "Tokenizer_2": "cl100k_base",
    "Tokenizer_3": "deepseek-ai/DeepSeek-R1"
  },
  {
    "name": "Hviezdoslav",
    "input_text": "Bola žatva. Sotvaže hojná oťaž rosy\nobschla: zrazu zmihalo tisíc skvúcich kosí\nvôkol jasných po stráňach nad ozimín diely,\nnakovaných za svitu, leda prúžok biely\nnakazoval, kade má slnce zpoza holí —\n(lebo planý gazda to, ktorý kúva v poli;\nešte horší, oslou len čo vždy midlikuje:\nnepodlieha úroda, len sa chlpí, snuje).\nLigot zahral, hovorím — rozbresknutá zora —\nvšak i zhasol: do spľaslých klasnatého mora\nvĺn vtopil sa po lúči, kde úvrate zhusta;\nnebadať ho, iba čuť: jak sa valí, šústa\nbodrým koscom zajatá ručaj zo pšenice\nči lenivý žita prúd. Zhíkly prepelice\n,chau-chau‘ v strachu nemalom na ruch zakosenia,\nhybaj s ráznym ,podspodok!‘ prosto do jačmeňa!\nChriašteľ zvrieskal; svrčky zas, ach, tak smutno nôťa,\nnie div: keď im vezmú žeň, čo im zbude? Psota.",
    "Tokenizer_1": "gpt2",
    "Tokenizer_2": "cl100k_base",
    "Tokenizer_3": "custom:Hviezdo LLaMA CulturaX"
  }
]